{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubham-M-Rathod/bert-question-answer-bot/blob/main/bert_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "NOnpxAowRrK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d27eeed-db80-4e6b-e581-6c4b8668d5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pretrained BERTforQuestionAnswering"
      ],
      "metadata": {
        "id": "VruUT-kKZDmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = TFBertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXTlsySM3HjG",
        "outputId": "b3fc795e-9ffe-403f-b44e-a40a483263be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF -q"
      ],
      "metadata": {
        "id": "utEVPum8xKzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Input"
      ],
      "metadata": {
        "id": "YzT_Sougamw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = r\"\"\"Auroras are the result of disturbances in the magnetosphere caused by the solar wind. Major disturbances result from enhancements in the speed of the solar wind from coronal holes and coronal mass ejections. These disturbances alter the trajectories of charged particles in the magnetospheric plasma. These particles, mainly electrons and protons, precipitate into the upper atmosphere (thermosphere/exosphere). The resulting ionization and excitation of atmospheric constituents emit light of varying colour and complexity. The form of the aurora, occurring within bands around both polar regions, is also dependent on the amount of acceleration imparted to the precipitating particles.\"\"\"\n",
        "question =r\"\"\"What causes the different colors of Aurora\"\"\""
      ],
      "metadata": {
        "id": "R0whyhOz34Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text =  question + \" [SEP] \" + text\n",
        "input_encoded = tokenizer.encode(input_text)\n",
        "input = tf.constant(input_encoded)[None, :]  # ? Batch size 1 needed"
      ],
      "metadata": {
        "id": "n3OEEh0hEe3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_scores, end_scores = model(input)[:2] ### ? end_scores me add 1"
      ],
      "metadata": {
        "id": "DfhhnDODEsav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "startIdx = tf.math.argmax(start_scores[0],0).numpy()\n",
        "endIdx = tf.math.argmax(end_scores[0],0).numpy()+1"
      ],
      "metadata": {
        "id": "vIjVWG4RJjFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.math.argmax(end_scores[0],0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No3SrqXKeYPJ",
        "outputId": "8f126032-81fd-4ab7-9f63-b5c772ce3f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=31>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenizer.convert_ids_to_tokens(input_encoded)\n",
        "print(' '.join(tokenized_text[startIdx : endIdx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6vC3dE2KJtL",
        "outputId": "cc67276b-5256-42cd-a64a-a6b03b9bb482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ion ##ization and ex ##cit ##ation of atmospheric constituents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF input"
      ],
      "metadata": {
        "id": "Xjy-wQ1zayhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdfs=['/content/nnfl.pdf']\n",
        "import fitz\n",
        "import re\n",
        "text = ''\n",
        "for pdf in pdfs:\n",
        "    doc = fitz.open(pdf)\n",
        "    for page in doc:\n",
        "        text += page.get_text()"
      ],
      "metadata": {
        "id": "GoLjMFG0xUrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess(s):\n",
        "  processed_sentence = re.sub('\\n', ' ', s)\n",
        "  processed_sentence = re.sub('ï¿½', '', processed_sentence)\n",
        "  # processed_sentence =  ' '.join([word for word in processed_sentence if word.isalnum()])\n",
        "  return processed_sentence"
      ],
      "metadata": {
        "id": "G7wOrRVIyRXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text = preprocess(text)"
      ],
      "metadata": {
        "id": "89byFxUqzmQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text"
      ],
      "metadata": {
        "id": "sMt5Iy-lF9UE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "4164fe6a-3c02-44a9-dd10-e8c76773a6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Name : Shubham Rathod UID : 2021600056 Batch : C Branch : CSE(AI/ML) Course Code : AI304 Aim : To design and implement ANFIS model for a given problem statement Theory : An adaptive neuro-fuzzy inference system or adaptive network-based fuzzy inference system (ANFIS) is a kind of artificial neural network that is based on TakagiâSugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IFâTHEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithms. It has uses in intelligent situational awareness energy management systems. Program : lab_completion = [0, 50, 100] attendance = [0, 50, 100] prev_lab_score = [0, 50, 100] def lab_completion_membership(x): ans = [0] * 3 if lab_completion[0] <= x <= lab_completion[1]: ans[0] = (lab_completion[1] - x) / (lab_completion[1] - lab_completion[0]) ans[1] = (x - lab_completion[0]) / (lab_completion[1] - lab_completion[0]) elif lab_completion[1] <= x <= lab_completion[2]: ans[1] = (lab_completion[2] - x) / (lab_completion[2] - lab_completion[1]) ans[2] = (x - lab_completion[1]) / (lab_completion[2] - lab_completion[1]) return ans def attendance_membership(x): ans = [0] * 3 if attendance[0] <= x <= attendance[1]: ans[0] = (attendance[1] - x) / (attendance[1] - attendance[0]) ans[1] = (x - attendance[0]) / (attendance[1] - attendance[0]) elif attendance[1] <= x <= attendance[2]: ans[1] = (attendance[2] - x) / (attendance[2] - attendance[1]) ans[2] = (x - attendance[1]) / (attendance[2] - attendance[1]) return ans def prev_lab_score_membership(x): ans = [0] * 3 if prev_lab_score[0] <= x <= prev_lab_score[1]: ans[0] = (prev_lab_score[1] - x) / (prev_lab_score[1] - prev_lab_score[0]) ans[1] = (x - prev_lab_score[0]) / (prev_lab_score[1] - prev_lab_score[0]) elif prev_lab_score[1] <= x <= prev_lab_score[2]: ans[1] = (prev_lab_score[2] - x) / (prev_lab_score[2] - prev_lab_score[1]) ans[2] = (x - prev_lab_score[1]) / (prev_lab_score[2] - prev_lab_score[1]) return ans def fuzzy_rule_based_model(labc, attn, prevl): if labc < 35 and attn < 40 and prevl < 40: return 0.2, 0.15, 0.14 elif labc < 65 and attn < 68 and prevl < 80: return 0.27, 0.21, 0.19 elif labc < 90 and attn < 90 and prevl < 90: return 0.3, 0.29, 0.21 else: return 0.4, 0.31, 0.29 def anfis(labc, attn, prevl): print('lab_completion: ', labc) print('attendance: ', attn) print('prev_lab_score: ', prevl,end='\\\\n\\\\n') layer1_nodes = [lab_completion_membership(labc), attendance_membership(attn), prev_lab_score_membership(prevl)] print('layer 1 output: ', layer1_nodes) layer2_nodes = [] for i in range(3): layer2_nodes.append(max(layer1_nodes[0][i], layer1_nodes[1][i], layer1_nodes[2][i])) print('layer 2 output: ', layer2_nodes) total_layer2_output = sum(layer2_nodes) layer3_nodes = [] for i in range(3): layer3_nodes.append(layer2_nodes[i] / total_layer2_output) print('layer 3 output: ', layer3_nodes) layer4_nodes = [] p, q, r = fuzzy_rule_based_model(labc, attn, prevl) # print('p, q, r: ', p, q, r) for i in range(3): layer4_nodes.append(layer3_nodes[i] * (p*labc + q*attn + r*prevl)) print('layer 4 output: ', layer4_nodes) layer5_node = sum(layer4_nodes) print('layer 5 output: ', layer5_node,end='\\\\n\\\\n') print('student marks:', layer5_node) anfis(90, 90, 100) Output: Conclusion : 1) The lab experiment demonstrates the application of an ANFIS model to predict student marks based on lab completion, attendance, and previous lab scores. 2) The model is capable of handling non-linear relationships and noisy data, making it a powerful tool for predictive modeling. 3) The results of the experiment could be used to improve the teaching and learning process by providing a more accurate prediction of student marks based on their performance in the lab and their attendance. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question =r\"\"\"What is the aim\"\"\""
      ],
      "metadata": {
        "id": "oLiHVZE22TkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text =  question + \" [SEP] \" + text[:500]\n",
        "input_encoded = tokenizer.encode(input_text)\n",
        "input = tf.constant(input_encoded)[None, :]  # ? Batch size 1 needed"
      ],
      "metadata": {
        "id": "8XCv2OA-b-A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# For your pdf text ###########\n",
        "tokenized_text = tokenizer.convert_ids_to_tokens(input_encoded)\n",
        "start_scores, end_scores = model(input)[:2]### ? end_scores me add 1"
      ],
      "metadata": {
        "id": "21_2SaNm3Pxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "startIdx = tf.math.argmax(start_scores[0],0).numpy()\n",
        "endIdx = tf.math.argmax(end_scores[0],0).numpy()+1"
      ],
      "metadata": {
        "id": "eUkFzT1YdbGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer=(' '.join(tokenized_text[startIdx : endIdx]))\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK_C1oF2cv8t",
        "outputId": "f48468d5-4312-44cb-cd78-4bba7edc5800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to design and implement an ##fi ##s model for a given problem statement theory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}